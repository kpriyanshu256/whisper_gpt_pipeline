{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J_QHBvDEk8D"
      },
      "source": [
        "Huggingface (HF) Transformers provides [scripts](https://github.com/huggingface/transformers/tree/main/examples) to train models for different tasks. In this notebook, we learn how to train a Causal Language Model (CLM) in a quick manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5S7Rdq9FO-j"
      },
      "source": [
        "First of all, we download and save the [script](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py) for training a CLM provided by Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXBAfubO8FVZ",
        "outputId": "e3546b9b-c5fe-462a-a209-4688853c4385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 25029  100 25029    0     0  75845      0 --:--:-- --:--:-- --:--:-- 75616\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py > run_clm.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miun9qLVFbM0"
      },
      "source": [
        "We install the necessay `datasets` and `evaluate` libraries which will be needed for loading the data and validating our trained model.\n",
        "\n",
        "We can also provide custom training and validation files as per this comment in the training script (line 261)\n",
        "\n",
        "\n",
        "\n",
        "> Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below) or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/ (the dataset will be downloaded automatically from the datasets Hub). For CSV/JSON files, this script will use the column called 'text' or the first column if no column called 'text' is found. You can easily tweak this behavior (see below).\n",
        "\n",
        "Therefore, having a `text` column in our HF dataset is beneficial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSWeUAhG9gMw"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q\n",
        "!pip install evaluate -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91O6gu9LaDzv"
      },
      "source": [
        "The script requires Transformers to be installed from source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTuoT1nJ94vY",
        "outputId": "8459373b-7031-489b-b5e8-ab3d3e856da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQoTDLJfaMTG"
      },
      "source": [
        "We now train our model using the `run_clm.py` script and pass the arguments as per our choice. (Check https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments for the complete list of training arguments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTYob0W8biv7"
      },
      "source": [
        "We use the transcripts of videos from the \"The Verge\" Youtube channel, hosted at https://huggingface.co/datasets/kpriyanshu256/whisper-transcripts . The dataset has a `text` column which enables us to use the script without any modifications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JvDZJ0Z8d0u",
        "outputId": "400dfa56-862c-4d42-94e5-a0d66437bff0"
      },
      "outputs": [],
      "source": [
        "!python run_clm.py --model_name_or_path distilgpt2 --dataset_name kpriyanshu256/whisper-transcripts\\\n",
        " --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --output_dir /tmp/test-clm --validation_split_percentage 20 --fp16 True\\\n",
        "  --num_train_epochs 1 --block_size 256"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
