{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXBAfubO8FVZ",
        "outputId": "e3546b9b-c5fe-462a-a209-4688853c4385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 25029  100 25029    0     0  75845      0 --:--:-- --:--:-- --:--:-- 75616\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_clm.py > run_clm.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "!pip install evaluate -q"
      ],
      "metadata": {
        "id": "PSWeUAhG9gMw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTuoT1nJ94vY",
        "outputId": "8459373b-7031-489b-b5e8-ab3d3e856da0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_clm.py --model_name_or_path distilgpt2 --dataset_name kpriyanshu256/whisper-transcripts\\\n",
        " --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --do_train --do_eval --output_dir /tmp/test-clm --validation_split_percentage 20 --fp16 True\\\n",
        "  --num_train_epochs 1 --block_size 256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JvDZJ0Z8d0u",
        "outputId": "400dfa56-862c-4d42-94e5-a0d66437bff0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/test-clm/runs/Oct20_03-30-25_36bbf9b60ad3,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=/tmp/test-clm,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/test-clm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "INFO:datasets.utils.file_utils:https://huggingface.co/datasets/kpriyanshu256/whisper-transcripts/resolve/main/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqdrt229z\n",
            "Downloading readme: 100% 365/365 [00:00<00:00, 233kB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/kpriyanshu256/whisper-transcripts/resolve/main/README.md in cache at /root/.cache/huggingface/datasets/downloads/7c80ae55191dc09d8b7c47e06c9ab6a352080a94e11048a8b9d8053580de1ff0.f964f340aeb1f0010df371b84ff99cdc71b19ff08dce0f8308e504aaea84b97b\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/7c80ae55191dc09d8b7c47e06c9ab6a352080a94e11048a8b9d8053580de1ff0.f964f340aeb1f0010df371b84ff99cdc71b19ff08dce0f8308e504aaea84b97b\n",
            "WARNING:datasets.builder:Using custom data configuration kpriyanshu256--whisper-transcripts-97766ab6b9b00466\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.7/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "Downloading and preparing dataset json/kpriyanshu256--whisper-transcripts to /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n",
            "INFO:datasets.builder:Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]INFO:datasets.utils.file_utils:https://huggingface.co/datasets/kpriyanshu256/whisper-transcripts/resolve/07f9f82caf21efbd0b758803ab25925e4d112866/the-verge.jsonl not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpg_cozhug\n",
            "\n",
            "Downloading data:   0% 0.00/64.5M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 20.5k/64.5M [00:00<10:54, 98.6kB/s]\u001b[A\n",
            "Downloading data:   0% 56.3k/64.5M [00:00<07:37, 141kB/s] \u001b[A\n",
            "Downloading data:   0% 143k/64.5M [00:00<04:02, 265kB/s] \u001b[A\n",
            "Downloading data:   0% 317k/64.5M [00:00<02:10, 490kB/s]\u001b[A\n",
            "Downloading data:   1% 648k/64.5M [00:01<01:12, 885kB/s]\u001b[A\n",
            "Downloading data:   2% 1.34M/64.5M [00:01<00:36, 1.72MB/s]\u001b[A\n",
            "Downloading data:   4% 2.71M/64.5M [00:01<00:18, 3.28MB/s]\u001b[A\n",
            "Downloading data:   8% 5.25M/64.5M [00:01<00:09, 6.09MB/s]\u001b[A\n",
            "Downloading data:  13% 8.33M/64.5M [00:01<00:06, 8.78MB/s]\u001b[A\n",
            "Downloading data:  19% 12.4M/64.5M [00:02<00:04, 11.9MB/s]\u001b[A\n",
            "Downloading data:  24% 15.6M/64.5M [00:02<00:03, 13.0MB/s]\u001b[A\n",
            "Downloading data:  30% 19.6M/64.5M [00:02<00:03, 14.8MB/s]\u001b[A\n",
            "Downloading data:  37% 23.7M/64.5M [00:02<00:02, 16.1MB/s]\u001b[A\n",
            "Downloading data:  43% 27.7M/64.5M [00:02<00:02, 16.9MB/s]\u001b[A\n",
            "Downloading data:  49% 31.6M/64.5M [00:03<00:01, 17.4MB/s]\u001b[A\n",
            "Downloading data:  55% 35.6M/64.5M [00:03<00:01, 17.8MB/s]\u001b[A\n",
            "Downloading data:  61% 39.6M/64.5M [00:03<00:01, 18.1MB/s]\u001b[A\n",
            "Downloading data:  68% 43.6M/64.5M [00:03<00:01, 18.3MB/s]\u001b[A\n",
            "Downloading data:  72% 46.4M/64.5M [00:04<00:01, 16.8MB/s]\u001b[A\n",
            "Downloading data:  78% 50.4M/64.5M [00:04<00:00, 17.4MB/s]\u001b[A\n",
            "Downloading data:  84% 54.1M/64.5M [00:04<00:00, 17.4MB/s]\u001b[A\n",
            "Downloading data:  90% 58.1M/64.5M [00:04<00:00, 17.8MB/s]\u001b[A\n",
            "Downloading data: 100% 64.5M/64.5M [00:04<00:00, 13.2MB/s]\n",
            "INFO:datasets.utils.file_utils:storing https://huggingface.co/datasets/kpriyanshu256/whisper-transcripts/resolve/07f9f82caf21efbd0b758803ab25925e4d112866/the-verge.jsonl in cache at /root/.cache/huggingface/datasets/downloads/604a2c97b2840ff85618134f016c7293a02d3d28047b9d9a3d5e9ff190ea3d11\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/604a2c97b2840ff85618134f016c7293a02d3d28047b9d9a3d5e9ff190ea3d11\n",
            "Downloading data files: 100% 1/1 [00:08<00:00,  8.73s/it]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 769.17it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 207.53it/s]\n",
            "WARNING:datasets.builder:Using custom data configuration kpriyanshu256--whisper-transcripts-97766ab6b9b00466\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.7/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "WARNING:datasets.builder:Using custom data configuration kpriyanshu256--whisper-transcripts-97766ab6b9b00466\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.7/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab\n",
            "Downloading: 100% 762/762 [00:00<00:00, 588kB/s]\n",
            "[INFO|configuration_utils.py:653] 2022-10-20 03:30:53,651 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/c3772e6d13ecdaf8d1105055f7c89becd6e37590/config.json\n",
            "[INFO|configuration_utils.py:705] 2022-10-20 03:30:53,652 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:420] 2022-10-20 03:30:54,591 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:653] 2022-10-20 03:30:55,508 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/c3772e6d13ecdaf8d1105055f7c89becd6e37590/config.json\n",
            "[INFO|configuration_utils.py:705] 2022-10-20 03:30:55,508 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading: 100% 1.04M/1.04M [00:01<00:00, 769kB/s]\n",
            "Downloading: 100% 456k/456k [00:01<00:00, 401kB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:01<00:00, 980kB/s]\n",
            "[INFO|tokenization_utils_base.py:1776] 2022-10-20 03:31:07,864 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/c3772e6d13ecdaf8d1105055f7c89becd6e37590/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1776] 2022-10-20 03:31:07,864 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/c3772e6d13ecdaf8d1105055f7c89becd6e37590/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1776] 2022-10-20 03:31:07,864 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/c3772e6d13ecdaf8d1105055f7c89becd6e37590/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1776] 2022-10-20 03:31:07,864 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1776] 2022-10-20 03:31:07,865 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1776] 2022-10-20 03:31:07,865 >> loading file tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:653] 2022-10-20 03:31:07,865 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/c3772e6d13ecdaf8d1105055f7c89becd6e37590/config.json\n",
            "[INFO|configuration_utils.py:705] 2022-10-20 03:31:07,866 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"distilgpt2\",\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 6,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.24.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "Downloading: 100% 353M/353M [00:05<00:00, 66.5MB/s]\n",
            "[INFO|modeling_utils.py:2156] 2022-10-20 03:31:14,165 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--distilgpt2/snapshots/c3772e6d13ecdaf8d1105055f7c89becd6e37590/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2606] 2022-10-20 03:31:15,466 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2615] 2022-10-20 03:31:15,466 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/3 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3521] 2022-10-20 03:31:19,584 >> Token indices sequence length is longer than the specified maximum sequence length for this model (2169 > 1024). Running this sequence through the model will result in indexing errors\n",
            "[WARNING|run_clm.py:409] 2022-10-20 03:31:19,585 >> ^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits before being passed to the model.\n",
            "INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4c491101d97de5b2.arrow\n",
            "Running tokenizer on dataset:  67% 2/3 [00:08<00:04,  4.40s/ba]\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-6c17a6413226cdfd.arrow\n",
            "Running tokenizer on dataset:   0% 0/1 [00:02<?, ?ba/s]\n",
            "Grouping texts in chunks of 256:   0% 0/3 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e0f134e10b48e38d.arrow\n",
            "Grouping texts in chunks of 256:  67% 2/3 [00:03<00:01,  1.98s/ba]\n",
            "Grouping texts in chunks of 256:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-97766ab6b9b00466/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-fbcdfba5fb3997bb.arrow\n",
            "Grouping texts in chunks of 256:   0% 0/1 [00:01<?, ?ba/s]\n",
            "Downloading builder script: 100% 4.20k/4.20k [00:00<00:00, 2.79MB/s]\n",
            "[INFO|trainer.py:557] 2022-10-20 03:31:41,216 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1607] 2022-10-20 03:31:41,230 >> ***** Running training *****\n",
            "[INFO|trainer.py:1608] 2022-10-20 03:31:41,230 >>   Num examples = 14742\n",
            "[INFO|trainer.py:1609] 2022-10-20 03:31:41,230 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1610] 2022-10-20 03:31:41,230 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1611] 2022-10-20 03:31:41,230 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1612] 2022-10-20 03:31:41,230 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1613] 2022-10-20 03:31:41,230 >>   Total optimization steps = 1843\n",
            "{'loss': 3.4981, 'learning_rate': 3.648941942485079e-05, 'epoch': 0.27}\n",
            " 27% 500/1843 [01:30<04:00,  5.58it/s][INFO|trainer.py:2671] 2022-10-20 03:33:12,112 >> Saving model checkpoint to /tmp/test-clm/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2022-10-20 03:33:12,113 >> Configuration saved in /tmp/test-clm/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-10-20 03:33:13,109 >> Model weights saved in /tmp/test-clm/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2126] 2022-10-20 03:33:13,110 >> tokenizer config file saved in /tmp/test-clm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2133] 2022-10-20 03:33:13,110 >> Special tokens file saved in /tmp/test-clm/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 3.4322, 'learning_rate': 2.292457948996202e-05, 'epoch': 0.54}\n",
            " 54% 1000/1843 [03:05<02:34,  5.47it/s][INFO|trainer.py:2671] 2022-10-20 03:34:46,438 >> Saving model checkpoint to /tmp/test-clm/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2022-10-20 03:34:46,439 >> Configuration saved in /tmp/test-clm/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-10-20 03:34:47,200 >> Model weights saved in /tmp/test-clm/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2126] 2022-10-20 03:34:47,200 >> tokenizer config file saved in /tmp/test-clm/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2133] 2022-10-20 03:34:47,200 >> Special tokens file saved in /tmp/test-clm/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 3.4131, 'learning_rate': 9.359739555073251e-06, 'epoch': 0.81}\n",
            " 81% 1500/1843 [04:40<01:03,  5.42it/s][INFO|trainer.py:2671] 2022-10-20 03:36:21,771 >> Saving model checkpoint to /tmp/test-clm/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2022-10-20 03:36:21,772 >> Configuration saved in /tmp/test-clm/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-10-20 03:36:22,631 >> Model weights saved in /tmp/test-clm/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2126] 2022-10-20 03:36:22,632 >> tokenizer config file saved in /tmp/test-clm/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2133] 2022-10-20 03:36:22,632 >> Special tokens file saved in /tmp/test-clm/checkpoint-1500/special_tokens_map.json\n",
            "100% 1843/1843 [05:47<00:00,  5.72it/s][INFO|trainer.py:1852] 2022-10-20 03:37:28,741 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 347.512, 'train_samples_per_second': 42.422, 'train_steps_per_second': 5.303, 'train_loss': 3.436913492882698, 'epoch': 1.0}\n",
            "100% 1843/1843 [05:47<00:00,  5.30it/s]\n",
            "[INFO|trainer.py:2671] 2022-10-20 03:37:28,744 >> Saving model checkpoint to /tmp/test-clm\n",
            "[INFO|configuration_utils.py:447] 2022-10-20 03:37:28,745 >> Configuration saved in /tmp/test-clm/config.json\n",
            "[INFO|modeling_utils.py:1624] 2022-10-20 03:37:29,635 >> Model weights saved in /tmp/test-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2126] 2022-10-20 03:37:29,636 >> tokenizer config file saved in /tmp/test-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2133] 2022-10-20 03:37:29,636 >> Special tokens file saved in /tmp/test-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     3.4369\n",
            "  train_runtime            = 0:05:47.51\n",
            "  train_samples            =      14742\n",
            "  train_samples_per_second =     42.422\n",
            "  train_steps_per_second   =      5.303\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:2922] 2022-10-20 03:37:29,732 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2924] 2022-10-20 03:37:29,732 >>   Num examples = 4788\n",
            "[INFO|trainer.py:2927] 2022-10-20 03:37:29,732 >>   Batch size = 8\n",
            "100% 599/599 [00:41<00:00, 14.32it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_accuracy           =     0.3422\n",
            "  eval_loss               =     3.3568\n",
            "  eval_runtime            = 0:00:41.90\n",
            "  eval_samples            =       4788\n",
            "  eval_samples_per_second =    114.248\n",
            "  eval_steps_per_second   =     14.293\n",
            "  perplexity              =    28.6984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zh9H4zyInV5W"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}